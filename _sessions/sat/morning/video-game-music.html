---
layout: session
title: "Video Game Music: Analyzing Interactivity"
---

<h1>Video Game Music: Analyzing Interactivity <span class="room">(Grand F)</span></h1>
<h2 class="chair">Steven Beverburg Reale (Youngstown State University), Chair</h2>

<p class="author">Steven Beverburg Reale (Youngstown State University)</p>
<p class="title">A Musical Atlas of Hyrule: Video Games and Spatial
  Listening</p>

<p class="author">Julianne Grasso (University of Chicago)</p>
<p class="title">Music in the Time of Video Games: an approach to musically
  Mediated Gameplay</p>

<p class="author">Elizabeth Medina-Grey (Humboldt State University)</p>
<p class="title">Sound Effects as Music (or Not): Earcons and Auditory Icons
  in Video Games</p>

<p class="author">William R. Ayers (University of Cincinnati, College-Conservatory of Music)	</p>
<p class="title">Analyzing Narrative in Video Game Music: Topic Theory and
  Modular Design</p>


<h2>Abstracts</h2>

<p class="abs-title">A Musical Atlas of Hyrule: Video Games and Spatial
  Listening</p>
<p class="abs-author">Steven Beverburg Reale (Youngstown State University)</p>

<p>Lewin’s “transformational attitude” posits a first-person agent moving through a musical composition with an analytical network serving as a map. In this view, transformational listening relies on a metaphor conceptualizing in spatial and often achronological terms the temporal, linear logic under which music is commonly understood to unfold. But in video games, interactivity creates possibilities for indeterminate storytelling; as a result, nonlinear musical experiences are common. Moreover, many video games establish virtual worlds with internally-consistent geographies that promote highly spatial gameplay experiences; by associating specific musical cues with specific game-world locations, composers can promote a spatial listening experience for the player.</p>
<p>The music from Nintendo’s <i>Legend of Zelda </i>franchise has received considerable attention from game sound scholars. <i>The Ocarina of Time </i>(1998) introduced to the franchise the now-standard conceit of an in-game instrument on which the player “performs” melodies that influence the game world, Hyrule. The game’s titular ocarina provides a limited set of pitches from which many of the game’s principle melodies are derived, creating both a kind of “tonic sonority” as well as a “pivot set” for much of the game’s score. Since specific regions have specific musical accompaniments, a transformational network exists that is isographic to the geography of Hyrule. The score thus articulates a musical geography through which players traverse while directing Link through the game world, collapsing the metaphorical space between the music and the analytical network describing it.</p>

<p class="abs-title">Music in the Time of Video Games: an approach to musically
  Mediated Gameplay</p>
<p class="abs-author">Julianne Grasso (University of Chicago)</p>

<p>The relation of music to temporality is a familiar subject in music theory, not least since Jonathan Kramer’s <i>The Time of Music </i>(1988) grappled with several points of intersection in Western art music and beyond. Yet even if we take for granted Kramer’s claim that music acquires meaning through time, this notion has yet to be explored in video game music analysis. Time in video games generally remains under-theorized beyond Jesper Juul’s “Introduction to Game Time” (2004), which posits a duality between<i> </i>a game’s pre-scripted unfolding of events (<i>event time</i>) and the “real” time it takes to play them (<i>play time</i>)—a distinction that draws on classic narrative divisions between “story” and “discourse”. Expanding on Juul’s categories, I posit further distinctions within <i>event time </i>to encompass musical mappings of temporality in role-playing and adventure genres, from narratively-oriented themes to the dynamic musical events that coincide with player action. Within this framework, I draw from cognitive musicology to analyze how these temporal scales are iterated through music, showing how music can guide player behavior by enacting the temporality of appropriate actions. Through iterating multiple temporal scales, music in video games is more than sonic decoration—it becomes a script for gameplay itself.</p>

<p class="abs-title">Sound Effects as Music (or Not): Earcons and Auditory Icons
  in Video Games</p>
<p class="abs-author">Elizabeth Medina-Grey </p>

<p>Defining “music” in video games may at first seem to be unproblematic. Familiarly, music involves sustained organization of sound across time according to structures of pitch and/or rhythm; in video games, the score that accompanies gameplay typifies this category. However, many sound effects in games—brief sounds tied to gameplay actions or events through what Karen Collins (2013) has called <i>kinesonic synchresis</i>—are also <i>musical</i> in that they contain pitch or rhythm; Collins, Reale (2014), and others have pointed out that sound effects can even become part of a game’s music in particular contexts. Sound effects provide an important channel of communication between player and game. When such sounds become <i>music</i>, what implications might this status have for interactive gameplay?</p>
<p>This paper introduces a framework in which to consider the relative musicality of sound effects in games. First, this paper adopts a distinction from the field of Human-Computer Interaction between two types of sound effects that convey information to the user: <i>auditory icons</i>—naturalistic sounds with pre-existing associations—and <i>earcons</i>—abstract sequences of tones. Individual earcons are typically musical and auditory icons are typically non-musical, but neither are, by themselves, music. Next, this paper considers sound effects in their wider context, and especially together with a game’s musical score, drawing on the author’s (2014) analytical method for gauging smoothness between layers of game soundtracks. This paper examines several examples from various games to suggest ways in which sound effects’ status as music—or not—might subtly or significantly impact gameplay.</p>

<p class="abs-title">Analyzing Narrative in Video Game Music: Topic Theory and
  Modular Design</p>
<p class="abs-author">William R. Ayers (University of Cincinnati, College-Conservatory of Music)	</p>

<p>Examining the narrative structure of music in video games is often a challenge due to the dynamic quality of the medium. Cues are frequently triggered by player actions, providing a degree of unpredictability to the overall musical design. Though recent studies by Elizabeth Medina-Gray and Winifred Phillips have provided tools for analyzing this open-ended structure, many aspects of narrative structure in video game music have not yet been explored. In this presentation I will expand current methodologies for dealing with modular video game music to incorporate the materials of topic theory, specifically considering examples from the <i>Batman: Arkham</i> series of video games.</p>
<p>Like many video game scores, the music from the <i>Arkham</i> series draws on the language of Hollywood Neo-Romanticism, including extant semiotic techniques from the nineteenth century. Musical modules in this series are often coded with topical references such as the ombra, the march, or the fanfare; these references may indicate current game states or suggest future events. User interactivity plays a major role in this semiotic narration. By analyzing the topical content of interacting modules as they are triggered by a player’s actions, we can observe an emergent musical narrative which materializes in conjunction with gameplay situations such as exploration, combat, and victory. Topic theory can display individual modules as integral parts in this emergent narrative and can allow for the examination of deeper structures in the music of video games.</p>
